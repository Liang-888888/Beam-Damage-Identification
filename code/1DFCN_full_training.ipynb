{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44563c45-59e0-4169-9849-9fcab3f0d469",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import time\n",
    "import json\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class FullScaleConfig:\n",
    "\n",
    "    DATA_DIRS = {\n",
    "        'train': './dataset/train',   \n",
    "        'val': './dataset/val',        \n",
    "        'test': './dataset/test'       \n",
    "    }\n",
    "\n",
    "    TRAIN_SAMPLES = None      # Noneè¡¨ç¤ºä½¿ç”¨å…¨éƒ¨\n",
    "    VAL_SAMPLES = None        \n",
    "    TEST_SAMPLES = None       \n",
    "    \n",
    "\n",
    "    INPUT_DIM = 255\n",
    "    OUTPUT_DIM = 50\n",
    "    HIDDEN_DIMS = [512, 256, 128, 64]  \n",
    "    DROPOUT_RATE = 0.35       \n",
    "\n",
    "    BATCH_SIZE = 64          \n",
    "    EPOCHS = 300              \n",
    "    LEARNING_RATE = 0.0015    \n",
    "    WEIGHT_DECAY = 1e-4       \n",
    "\n",
    "    EARLY_STOP_PATIENCE = 50  \n",
    "\n",
    "    OPTIMIZER = 'AdamW'\n",
    "    USE_GRADIENT_CLIP = True\n",
    "    GRADIENT_CLIP_VAL = 1.0   \n",
    "\n",
    "    NUM_WORKERS = 0           \n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    RUN_ID = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    SAVE_DIR = Path(f'./full_scale_results/run_{RUN_ID}')\n",
    "    SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    def __init__(self):\n",
    "        print(\"=\" * 80)\n",
    "        print(\"å®Œæ•´è§„æ¨¡è®­ç»ƒé…ç½® - 90kè®­ç»ƒ + 15kéªŒè¯ + 5kæµ‹è¯•\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"æ•°æ®è§„æ¨¡: å®Œæ•´æ•°æ®é›†\")\n",
    "        print(f\"æ¨¡å‹æ¶æ„: {self.INPUT_DIM} â†’ {self.HIDDEN_DIMS} â†’ {self.OUTPUT_DIM}\")\n",
    "        print(f\" è¶…å‚æ•°: LR={self.LEARNING_RATE}, BS={self.BATCH_SIZE}, Dropout={self.DROPOUT_RATE}\")\n",
    "        print(f\"æœ€å¤§å‘¨æœŸ: {self.EPOCHS}, æ—©åœè€å¿ƒ={self.EARLY_STOP_PATIENCE}\")\n",
    "        print(f\"è®¾å¤‡: {self.DEVICE}\")\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            gpu_name = torch.cuda.get_device_name(0)\n",
    "            print(f\"GPU: {gpu_name}\")\n",
    "\n",
    "class SmartBeamDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_dir, max_samples=None, shuffle=True):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.max_samples = max_samples\n",
    "        self.shuffle = shuffle\n",
    "        self.samples = []\n",
    "        \n",
    "        print(f\"\\nåŠ è½½æ•°æ®ä»: {data_dir}\")\n",
    "        print(f\"   ç›®æ ‡æ ·æœ¬æ•°: {max_samples if max_samples else 'å…¨éƒ¨'}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        self._load_and_cache_data()\n",
    "        load_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"åŠ è½½å®Œæˆ: {len(self.samples):,} ä¸ªæ ·æœ¬\")\n",
    "    \n",
    "    def _load_and_cache_data(self):\n",
    "\n",
    "        batch_files = sorted(self.data_dir.glob('batch_*.pkl'))\n",
    "        \n",
    "        if not batch_files:\n",
    "            raise FileNotFoundError(f\"æœªæ‰¾åˆ°æ‰¹æ¬¡æ–‡ä»¶: {self.data_dir}\")\n",
    "        \n",
    "        print(f\"   å‘ç° {len(batch_files)} ä¸ªæ‰¹æ¬¡æ–‡ä»¶\")\n",
    "        \n",
    "        loaded_samples = 0\n",
    "        for file_idx, batch_file in enumerate(batch_files):\n",
    "\n",
    "            if self.max_samples and loaded_samples >= self.max_samples:\n",
    "                break\n",
    "            \n",
    "\n",
    "            with open(batch_file, 'rb') as f:\n",
    "                batch_data = pickle.load(f)\n",
    "\n",
    "            remaining = self.max_samples - loaded_samples if self.max_samples else len(batch_data)\n",
    "            samples_to_add = min(len(batch_data), remaining)\n",
    "\n",
    "            self.samples.extend(batch_data[:samples_to_add])\n",
    "            loaded_samples += samples_to_add\n",
    "\n",
    "            if (file_idx + 1) % 5 == 0 or (file_idx + 1) == len(batch_files):\n",
    "                print(f\"   [{file_idx+1:03d}/{len(batch_files)}] å·²åŠ è½½ {loaded_samples:,} æ ·æœ¬\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "\n",
    "        freq = torch.FloatTensor(sample['input']['frequencies'][:5])\n",
    "        mode_shapes = torch.FloatTensor(sample['input']['mode_shapes'][:5])\n",
    "        x = torch.cat([freq, mode_shapes.flatten()])\n",
    "        y = torch.FloatTensor(sample['output']['binary_labels'])\n",
    "        \n",
    "        return x, y\n",
    "\n",
    "class FullScaleFCN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, hidden_dims, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for i, h_dim in enumerate(hidden_dims):\n",
    "            layers.append(nn.Linear(prev_dim, h_dim))\n",
    "            layers.append(nn.BatchNorm1d(h_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            prev_dim = h_dim\n",
    "        \n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0.01)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class SimpleEarlyStopping:\n",
    "    def __init__(self, patience=20, min_delta=0.0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.early_stop = False\n",
    "    \n",
    "    def __call__(self, val_loss):\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            improved = True\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            improved = False\n",
    "            \n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        \n",
    "        return self.early_stop, improved\n",
    "\n",
    "def compute_accuracy(predictions, targets, threshold=0.5):\n",
    "    probs = torch.sigmoid(predictions)\n",
    "    binary_preds = (probs > threshold).float()\n",
    "    correct = (binary_preds == targets).float()\n",
    "    return correct.mean().item()\n",
    "\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device, config):\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for batch_idx, (batch_x, batch_y) in enumerate(loader):\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        batch_size = batch_x.size(0)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(batch_x)\n",
    "        loss = criterion(predictions, batch_y)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        if config.USE_GRADIENT_CLIP:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.GRADIENT_CLIP_VAL)\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * batch_size\n",
    "        total_acc += compute_accuracy(predictions, batch_y) * batch_size\n",
    "        total_samples += batch_size\n",
    "\n",
    "        if (batch_idx + 1) % 50 == 0:\n",
    "            avg_loss = total_loss / total_samples\n",
    "            avg_acc = total_acc / total_samples\n",
    "            print(f'    æ‰¹æ¬¡ {batch_idx+1:04d}/{len(loader)} | æŸå¤±: {avg_loss:.4f} | å‡†ç¡®ç‡: {avg_acc:.3f}')\n",
    "    \n",
    "    avg_loss = total_loss / total_samples\n",
    "    avg_acc = total_acc / total_samples\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            batch_size = batch_x.size(0)\n",
    "            \n",
    "            predictions = model(batch_x)\n",
    "            loss = criterion(predictions, batch_y)\n",
    "            \n",
    "            total_loss += loss.item() * batch_size\n",
    "            total_acc += compute_accuracy(predictions, batch_y) * batch_size\n",
    "            total_samples += batch_size\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    avg_acc = total_acc / total_samples\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "def test_model(model, config):\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"é˜¶æ®µ4: åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œæœ€ç»ˆè¯„ä¼°\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    test_dataset = SmartBeamDataset(\n",
    "        config.DATA_DIRS['test'],\n",
    "        max_samples=config.TEST_SAMPLES,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=config.NUM_WORKERS\n",
    "    )\n",
    "    \n",
    "    print(f\" æµ‹è¯•é›†: {len(test_dataset):,} ä¸ªæ ·æœ¬\")\n",
    "    \n",
    "    pos_weight = torch.tensor([5.0]).to(config.DEVICE)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    test_loss, test_acc = validate(model, test_loader, criterion, config.DEVICE)\n",
    "    \n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    damage_correct = 0\n",
    "    damage_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x, y = x.to(config.DEVICE), y.to(config.DEVICE)\n",
    "            outputs = model(x)\n",
    "            preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "    \n",
    "            correct += (preds == y).sum().item()\n",
    "            total += y.numel()\n",
    "    \n",
    "            damage_mask = (y == 1)\n",
    "            damage_correct += (preds[damage_mask] == 1).sum().item()\n",
    "            damage_total += damage_mask.sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    damage_recall = damage_correct / damage_total if damage_total > 0 else 0.0\n",
    "\n",
    "    \n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Damage Recall:\", damage_recall)\n",
    "\n",
    "    return test_loss, test_acc, test_dataset\n",
    "\n",
    "def main():\n",
    "    config = FullScaleConfig()\n",
    "\n",
    "    config_dict = {k: v for k, v in config.__dict__.items() \n",
    "                  if not k.startswith('_') and not callable(v)}\n",
    "    with open(config.SAVE_DIR / 'training_config.json', 'w') as f:\n",
    "        json.dump(config_dict, f, indent=2, default=str)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"é˜¶æ®µ1: åŠ è½½æ•°æ®é›†\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    train_dataset = SmartBeamDataset(\n",
    "        config.DATA_DIRS['train'],\n",
    "        max_samples=config.TRAIN_SAMPLES,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    val_dataset = SmartBeamDataset(\n",
    "        config.DATA_DIRS['val'],\n",
    "        max_samples=config.VAL_SAMPLES,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=config.NUM_WORKERS\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=config.NUM_WORKERS\n",
    "    )\n",
    "    \n",
    "    print(f\"\\næ•°æ®é›†ç»Ÿè®¡:\")\n",
    "    print(f\"   è®­ç»ƒé›†: {len(train_dataset):,} ä¸ªæ ·æœ¬, {len(train_loader)} ä¸ªæ‰¹æ¬¡\")\n",
    "    print(f\"   éªŒè¯é›†: {len(val_dataset):,} ä¸ªæ ·æœ¬, {len(val_loader)} ä¸ªæ‰¹æ¬¡\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"é˜¶æ®µ2: åˆå§‹åŒ–æ¨¡å‹\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    model = FullScaleFCN(\n",
    "        config.INPUT_DIM,\n",
    "        config.OUTPUT_DIM,\n",
    "        config.HIDDEN_DIMS,\n",
    "        dropout_rate=config.DROPOUT_RATE\n",
    "    ).to(config.DEVICE)\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\" æ¨¡å‹å‚æ•°: {total_params:,}\")\n",
    "    print(f\"   æ¨¡å‹å¤§å°: {total_params * 4 / 1024**2:.2f} MB\")\n",
    "\n",
    "    pos_weight = torch.tensor([5.0]).to(config.DEVICE)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    \n",
    "    if config.OPTIMIZER == 'AdamW':\n",
    "        optimizer = optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=config.LEARNING_RATE,\n",
    "            weight_decay=config.WEIGHT_DECAY\n",
    "        )\n",
    "    else:\n",
    "        optimizer = optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=config.LEARNING_RATE\n",
    "        )\n",
    "\n",
    "    early_stopping = SimpleEarlyStopping(\n",
    "        patience=config.EARLY_STOP_PATIENCE\n",
    "    )\n",
    "\n",
    "    history = {\n",
    "        'epoch': [],\n",
    "        'train_loss': [], 'train_acc': [],\n",
    "        'val_loss': [], 'val_acc': []\n",
    "    }\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    best_val_loss = float('inf')\n",
    "    best_epoch = 0\n",
    "    training_start_time = time.time()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"é˜¶æ®µ3: å¼€å§‹å®Œæ•´è§„æ¨¡è®­ç»ƒ\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    for epoch in range(1, config.EPOCHS + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch:03d}/{config.EPOCHS}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        print(\"è®­ç»ƒé˜¶æ®µ:\")\n",
    "        train_loss, train_acc = train_epoch(\n",
    "            model, train_loader, criterion, optimizer, config.DEVICE, config\n",
    "        )\n",
    "\n",
    "        print(\"éªŒè¯é˜¶æ®µ:\")\n",
    "        val_loss, val_acc = validate(\n",
    "            model, val_loader, criterion, config.DEVICE\n",
    "        )\n",
    "\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        total_time = time.time() - training_start_time\n",
    "\n",
    "        history['epoch'].append(epoch)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "\n",
    "        print(f\"\\nğŸ“Š Epoch {epoch:03d} æ€»ç»“:\")\n",
    "        print(f\"   è®­ç»ƒæŸå¤±: {train_loss:.4f} | è®­ç»ƒå‡†ç¡®ç‡: {train_acc:.3f}\")\n",
    "        print(f\"   éªŒè¯æŸå¤±: {val_loss:.4f} | éªŒè¯å‡†ç¡®ç‡: {val_acc:.3f}\")\n",
    "        print(f\"   å‘¨æœŸæ—¶é—´: {epoch_time:.1f}s | ç´¯è®¡æ—¶é—´: {total_time/60:.1f}åˆ†é’Ÿ\")\n",
    "\n",
    "        early_stop, improved = early_stopping(val_loss)\n",
    "\n",
    "        if improved or val_acc > best_val_acc:\n",
    "            best_val_acc = max(val_acc, best_val_acc)\n",
    "            best_val_loss = val_loss\n",
    "            best_epoch = epoch\n",
    "            \n",
    "            best_checkpoint = {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'train_acc': train_acc,\n",
    "                'val_loss': val_loss,\n",
    "                'val_acc': val_acc,\n",
    "                'history': history\n",
    "            }\n",
    "            \n",
    "            torch.save(best_checkpoint, config.SAVE_DIR / 'best_model.pth')\n",
    "            print(f\"  ä¿å­˜æœ€ä½³æ¨¡å‹ @ Epoch {epoch}: éªŒè¯å‡†ç¡®ç‡ = {val_acc:.3f}\")\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            checkpoint_path = config.SAVE_DIR / f'checkpoint_epoch_{epoch:03d}.pth'\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'val_acc': val_acc\n",
    "            }, checkpoint_path)\n",
    "            print(f\"  æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}\")\n",
    "\n",
    "        if early_stop:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(\"è®­ç»ƒå®Œæˆ - æ—©åœè§¦å‘\")\n",
    "            print(f\"æœ€ä½³æ¨¡å‹ @ Epoch {best_epoch}: éªŒè¯å‡†ç¡®ç‡ = {best_val_acc:.3f}\")\n",
    "            print(f\"æ€»è®­ç»ƒæ—¶é—´: {total_time/60:.1f}åˆ†é’Ÿ\")\n",
    "            print(f\"{'='*60}\")\n",
    "            break\n",
    "\n",
    "    if not early_stopping.early_stop:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"è®­ç»ƒå®Œæˆ - è¾¾åˆ°æœ€å¤§å‘¨æœŸæ•°\")\n",
    "        print(f\"æœ€ä½³æ¨¡å‹ @ Epoch {best_epoch}: éªŒè¯å‡†ç¡®ç‡ = {best_val_acc:.3f}\")\n",
    "        print(f\"æ€»è®­ç»ƒæ—¶é—´: {total_time/60:.1f}åˆ†é’Ÿ\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"é˜¶æ®µ4: åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æœ€ä½³æ¨¡å‹\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    best_checkpoint = torch.load(config.SAVE_DIR / 'best_model.pth')\n",
    "    model.load_state_dict(best_checkpoint['model_state_dict'])\n",
    "\n",
    "    test_loss, test_acc, test_dataset = test_model(model, config)\n",
    "    \n",
    "    print(f\"\\næµ‹è¯•é›†ç»“æœ:\")\n",
    "    print(f\"   æµ‹è¯•æŸå¤±: {test_loss:.4f}\")\n",
    "    print(f\"   æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.3f}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ä¿å­˜æœ€ç»ˆç»“æœ\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    torch.save(model.state_dict(), config.SAVE_DIR / 'final_model.pth')\n",
    "\n",
    "    np.save(config.SAVE_DIR / 'training_history.npy', history)\n",
    "\n",
    "    test_results = {\n",
    "        'best_epoch': best_epoch,\n",
    "        'best_val_loss': float(best_val_loss),\n",
    "        'best_val_acc': float(best_val_acc),\n",
    "        'test_loss': float(test_loss),\n",
    "        'test_acc': float(test_acc),\n",
    "        'total_epochs': len(history['epoch']),\n",
    "        'total_training_time_minutes': total_time / 60,\n",
    "        'training_finished': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }\n",
    "    \n",
    "    with open(config.SAVE_DIR / 'test_results.json', 'w') as f:\n",
    "        json.dump(test_results, f, indent=2)\n",
    "\n",
    "    plot_training_curves(history, config.SAVE_DIR, test_results)\n",
    "    \n",
    "    print(f\"\\nğŸ‰ è®­ç»ƒå®Œæˆ!\")\n",
    "    print(f\" ç»“æœä¿å­˜åœ¨: {config.SAVE_DIR.absolute()}\")\n",
    "    print(f\" æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.3%} @ Epoch {best_epoch}\")\n",
    "    print(f\"æµ‹è¯•é›†å‡†ç¡®ç‡: {test_acc:.3%}\")\n",
    "\n",
    "    medium_val_acc = 0.824  \n",
    "    print(f\"\\n æ€§èƒ½å¯¹æ¯”:\")\n",
    "    print(f\"   ä¸­ç­‰è§„æ¨¡ (10kè®­ç»ƒ) éªŒè¯å‡†ç¡®ç‡: {medium_val_acc:.1%}\")\n",
    "    print(f\"   å®Œæ•´è§„æ¨¡ (90kè®­ç»ƒ) éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.1%}\")\n",
    "    print(f\"   å®Œæ•´è§„æ¨¡ (90kè®­ç»ƒ) æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.1%}\")\n",
    "    print(f\"   éªŒè¯é›†æå‡: {best_val_acc*100 - medium_val_acc*100:.1f}%\")\n",
    "    print(f\"   æµ‹è¯•é›†æœ€ç»ˆç»“æœ: {test_acc*100:.1f}%\")\n",
    "    \n",
    "    return model, history, test_results\n",
    "\n",
    "def plot_training_curves(history, save_dir, test_results):\n",
    "    epochs = history['epoch']\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "    fig.suptitle(f'Full data training (90k train + 15k val + 5k test)', fontsize=16, y=1.02)\n",
    "    \n",
    "    ax = axes[0, 0]\n",
    "    ax.plot(epochs, history['train_loss'], 'b-', linewidth=2, label='Training loss')\n",
    "    ax.plot(epochs, history['val_loss'], 'r-', linewidth=2, label='Verification loss')\n",
    "    ax.axvline(x=test_results['best_epoch'], color='g', linestyle='--', alpha=0.5, label='Best model')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title('Training and validation losses')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax = axes[0, 1]\n",
    "    ax.plot(epochs, history['train_acc'], 'b-', linewidth=2, label='Training accuracy')\n",
    "    ax.plot(epochs, history['val_acc'], 'r-', linewidth=2, label='Verify the accuracy rate')\n",
    "    ax.axvline(x=test_results['best_epoch'], color='g', linestyle='--', alpha=0.5)\n",
    "    ax.axhline(y=test_results['best_val_acc'], color='r', linestyle=':', alpha=0.5, \n",
    "               label=f'æœ€ä½³éªŒè¯: {test_results[\"best_val_acc\"]:.2%}')\n",
    "    ax.axhline(y=test_results['test_acc'], color='purple', linestyle='-.', alpha=0.5,\n",
    "               label=f'æµ‹è¯•å‡†ç¡®ç‡: {test_results[\"test_acc\"]:.2%}')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Accuracy rate')\n",
    "    ax.set_title('Train and verify the accuracy')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim([0.5, 1.0])\n",
    "    \n",
    "    ax = axes[1, 0]\n",
    "    scatter = ax.scatter(history['val_loss'], history['val_acc'], \n",
    "                        c=history['epoch'], cmap='viridis', \n",
    "                        s=30, alpha=0.7)\n",
    "    ax.set_xlabel('Verification loss')\n",
    "    ax.set_ylabel('Verify the accuracy rate')\n",
    "    ax.set_title('Loss vs. accuracy relationship')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.colorbar(scatter, ax=ax, label='Epoch')\n",
    "    \n",
    "    ax = axes[1, 1]\n",
    "    ax.axis('off')\n",
    "    \n",
    "    final_train_acc = history['train_acc'][-1] if history['train_acc'] else 0\n",
    "    final_val_acc = history['val_acc'][-1] if history['val_acc'] else 0\n",
    "    \n",
    "    summary_text = (\n",
    "        f\"è®­ç»ƒæ€»ç»“\\n\"\n",
    "        f\"========\\n\"\n",
    "        f\"æœ€ä½³å‘¨æœŸ: {test_results['best_epoch']}\\n\"\n",
    "        f\"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {test_results['best_val_acc']:.3%}\\n\"\n",
    "        f\"æµ‹è¯•å‡†ç¡®ç‡: {test_results['test_acc']:.3%}\\n\"\n",
    "        f\"æœ€ç»ˆè®­ç»ƒå‡†ç¡®ç‡: {final_train_acc:.3%}\\n\"\n",
    "        f\"æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {final_val_acc:.3%}\\n\"\n",
    "        f\"æ€»è®­ç»ƒå‘¨æœŸ: {len(epochs)}\\n\"\n",
    "        f\"è®­ç»ƒæ—¶é—´: {test_results['total_training_time_minutes']:.1f}åˆ†é’Ÿ\\n\"\n",
    "        f\"æ•°æ®è§„æ¨¡: 90kè®­ç»ƒ + 15kéªŒè¯ + 5kæµ‹è¯•\\n\"\n",
    "        f\"æ‰¹æ¬¡å¤§å°: 128\\n\"\n",
    "        f\"å­¦ä¹ ç‡: 0.0005\\n\"\n",
    "    )\n",
    "    \n",
    "    ax.text(0.1, 0.9, summary_text, transform=ax.transAxes,\n",
    "            fontsize=10, verticalalignment='top',\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plot_path = save_dir / 'training_curves.png'\n",
    "    plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    print(f\"è®­ç»ƒæ›²çº¿å·²ä¿å­˜: {plot_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 80)\n",
    "    print(\"æ¢ç»“æ„æŸä¼¤è¯†åˆ« - å®Œæ•´è§„æ¨¡è®­ç»ƒ\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    try:\n",
    "\n",
    "        print(\"\\nç³»ç»Ÿæ£€æŸ¥...\")\n",
    "        print(f\"PyTorchç‰ˆæœ¬: {torch.__version__}\")\n",
    "        print(f\"CUDAå¯ç”¨: {torch.cuda.is_available()}\")\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        else:\n",
    "            print(\"è­¦å‘Š: æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒ\")\n",
    "        \n",
    "\n",
    "        print(\"\\næ£€æŸ¥æ•°æ®è·¯å¾„...\")\n",
    "        for data_type, path in FullScaleConfig.DATA_DIRS.items():\n",
    "            data_path = Path(path)\n",
    "            if data_path.exists():\n",
    "                pkl_files = list(data_path.glob('batch_*.pkl'))\n",
    "                print(f\"âœ“ {data_type}: {len(pkl_files)}ä¸ªæ‰¹æ¬¡æ–‡ä»¶\")\n",
    "            else:\n",
    "                print(f\"âœ— {data_type}: è·¯å¾„ä¸å­˜åœ¨ {path}\")\n",
    "                raise FileNotFoundError(f\"æ•°æ®è·¯å¾„ä¸å­˜åœ¨: {path}\")\n",
    "        \n",
    "        print(\"\\nå¼€å§‹å®Œæ•´è§„æ¨¡è®­ç»ƒ...\")\n",
    "        \n",
    "        # è¿è¡Œè®­ç»ƒ\n",
    "        model, history, test_results = main()\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"ğŸ‰ è®­ç»ƒæˆåŠŸå®Œæˆ!\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        # ä¿å­˜é”™è¯¯ä¿¡æ¯\n",
    "        error_dir = Path('./training_errors')\n",
    "        error_dir.mkdir(exist_ok=True)\n",
    "        error_file = error_dir / f'error_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.txt'\n",
    "        \n",
    "        with open(error_file, 'w') as f:\n",
    "            f.write(f\"è®­ç»ƒé”™è¯¯: {e}\\n\")\n",
    "            f.write(f\"æ—¶é—´: {datetime.now()}\\n\")\n",
    "            f.write(\"\\nTraceback:\\n\")\n",
    "            f.write(traceback.format_exc())\n",
    "        \n",
    "        print(f\"é”™è¯¯è¯¦æƒ…å·²ä¿å­˜: {error_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Torch GPU)",
   "language": "python",
   "name": "torch_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
