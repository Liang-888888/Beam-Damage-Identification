{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e542b79-df2f-454a-8c4f-bb0a940f3999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_medium_cnn.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import time\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class MediumConfig:\n",
    "\n",
    "    DATA_DIRS = {\n",
    "        'train': './dataset/train',\n",
    "        'val': './dataset/val',\n",
    "        'test': './dataset/test'\n",
    "    }\n",
    "    \n",
    "    TRAIN_SAMPLES = 10000    \n",
    "    VAL_SAMPLES = 4000      \n",
    "    \n",
    "    INPUT_DIM = 255          \n",
    "    OUTPUT_DIM = 50          \n",
    "\n",
    "    BATCH_SIZE = 128         \n",
    "    EPOCHS = 150             \n",
    "    LEARNING_RATE = 0.0005   \n",
    "    WEIGHT_DECAY = 1e-4      \n",
    "    DROPOUT_RATE = 0.25      \n",
    "\n",
    "    OPTIMIZER = 'AdamW'\n",
    "    USE_GRADIENT_CLIP = False  \n",
    "    \n",
    "    LR_SCHEDULER = 'reduce_on_plateau'\n",
    "    LR_PATIENCE = 8\n",
    "    LR_FACTOR = 0.5\n",
    "\n",
    "    EARLY_STOP_PATIENCE = 50  \n",
    "    EARLY_STOP_MIN_DELTA = 0.0001\n",
    "\n",
    "    NUM_WORKERS = 0          \n",
    "    PIN_MEMORY = False\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    SAVE_DIR = Path('./medium_cnn_training')\n",
    "    SAVE_DIR.mkdir(exist_ok=True)\n",
    "    \n",
    "    def __init__(self):\n",
    "        print(\"=\" * 70)\n",
    "        print(\"ä¸­ç­‰è§„æ¨¡è®­ç»ƒé…ç½® - 1D CNNæ¶æ„\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"æ•°æ®è§„æ¨¡: {self.TRAIN_SAMPLES:,}è®­ç»ƒ + {self.VAL_SAMPLES:,}éªŒè¯\")\n",
    "        print(f\"æ¨¡å‹æ¶æ„: 1Då·ç§¯ç½‘ç»œ (å¤„ç†5é€šé“Ã—50ç‚¹ç©ºé—´ä¿¡å·)\")\n",
    "        print(f\"è®­ç»ƒå‚æ•°: LR={self.LEARNING_RATE}, BS={self.BATCH_SIZE}, Dropout={self.DROPOUT_RATE}\")\n",
    "        print(f\"æœ€å¤§å‘¨æœŸ: {self.EPOCHS}, æ—©åœè€å¿ƒ: {self.EARLY_STOP_PATIENCE}\")\n",
    "        print(f\"ä½¿ç”¨è®¾å¤‡: {self.DEVICE}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# ==================== æ™ºèƒ½æ•°æ®åŠ è½½å™¨ (ä¿æŒä¸å˜) ====================\n",
    "class SmartBeamDataset(Dataset):\n",
    "    def __init__(self, data_dir, max_samples=None, shuffle=True):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.max_samples = max_samples\n",
    "        self.shuffle = shuffle\n",
    "        self.samples = []\n",
    "        \n",
    "        print(f\"\\nåŠ è½½æ•°æ®ä»: {data_dir}\")\n",
    "        print(f\" ç›®æ ‡æ ·æœ¬æ•°: {max_samples if max_samples else 'å…¨éƒ¨'}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        self._load_and_cache_data()\n",
    "        load_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"åŠ è½½å®Œæˆ: {len(self.samples):,} ä¸ªæ ·æœ¬\")\n",
    "    \n",
    "    def _load_and_cache_data(self):\n",
    "        batch_files = sorted(self.data_dir.glob('batch_*.pkl'))\n",
    "        \n",
    "        if not batch_files:\n",
    "            raise FileNotFoundError(f\"æœªæ‰¾åˆ°æ‰¹æ¬¡æ–‡ä»¶: {self.data_dir}\")\n",
    "        \n",
    "        print(f\"   å‘ç° {len(batch_files)} ä¸ªæ‰¹æ¬¡æ–‡ä»¶\")\n",
    "        \n",
    "        loaded_samples = 0\n",
    "        for file_idx, batch_file in enumerate(batch_files):\n",
    "            if self.max_samples and loaded_samples >= self.max_samples:\n",
    "                break\n",
    "\n",
    "            with open(batch_file, 'rb') as f:\n",
    "                batch_data = pickle.load(f)\n",
    "\n",
    "            remaining = self.max_samples - loaded_samples if self.max_samples else len(batch_data)\n",
    "            samples_to_add = min(len(batch_data), remaining)\n",
    "\n",
    "            self.samples.extend(batch_data[:samples_to_add])\n",
    "            loaded_samples += samples_to_add\n",
    "\n",
    "            if (file_idx + 1) % 5 == 0 or (file_idx + 1) == len(batch_files):\n",
    "                print(f\"   [{file_idx+1:03d}/{len(batch_files)}] å·²åŠ è½½ {loaded_samples:,} æ ·æœ¬\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "\n",
    "        freq = torch.FloatTensor(sample['input']['frequencies'][:5])\n",
    "        mode_shapes = torch.FloatTensor(sample['input']['mode_shapes'][:5])\n",
    "        x = torch.cat([freq, mode_shapes.flatten()])\n",
    "        y = torch.FloatTensor(sample['output']['binary_labels'])\n",
    "\n",
    "        metadata = {\n",
    "            'sample_id': sample.get('sample_id', idx),\n",
    "            'damage_ratio': np.mean(sample['output']['binary_labels'])\n",
    "        }\n",
    "        \n",
    "        return x, y, metadata\n",
    "\n",
    "class BeamDamage1DCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    ä¸€ç»´å·ç§¯ç½‘ç»œ for ç©ºé—´æŸä¼¤è¯†åˆ«ã€‚\n",
    "    è®¾è®¡å“²å­¦ï¼šå°†å‰5é˜¶æŒ¯å‹è§†ä¸º5ä¸ªé€šé“çš„1Dä¿¡å·ï¼Œæ²¿æ¢é•¿è¿›è¡Œå·ç§¯ã€‚\n",
    "    \"\"\"\n",
    "    def __init__(self, n_freq=5, n_points=50, dropout_rate=0.25):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=n_freq, out_channels=32, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=32, out_channels=64, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=64, out_channels=128, kernel_size=7, dilation=2, padding=6), # dilation=2å¢å¤§æ„Ÿå—é‡\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        \n",
    "\n",
    "        self.freq_fc = nn.Sequential(\n",
    "            nn.Linear(n_freq, 16),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "\n",
    "        self.fusion_fc = nn.Sequential(\n",
    "            nn.Linear(128 + 16, 64), \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate * 0.5), \n",
    "            nn.Linear(64, 1)  \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        frequencies = x[:, :5]  # (batch, 5)\n",
    "        mode_shapes = x[:, 5:].view(batch_size, 5, -1)  # (batch, 5, 50)\n",
    "\n",
    "        spatial_feat = self.conv_block1(mode_shapes)  # -> (batch, 32, 50)\n",
    "        spatial_feat = self.conv_block2(spatial_feat) # -> (batch, 64, 50)\n",
    "        spatial_feat = self.conv_block3(spatial_feat) # -> (batch, 128, 50)\n",
    "\n",
    "        spatial_feat = spatial_feat.transpose(1, 2)\n",
    "\n",
    "        global_freq_feat = self.freq_fc(frequencies)  # -> (batch, 16)\n",
    "\n",
    "        global_freq_feat = global_freq_feat.unsqueeze(1).repeat(1, spatial_feat.size(1), 1)\n",
    "        \n",
    "\n",
    "        fused_feat = torch.cat([spatial_feat, global_freq_feat], dim=-1)  # (batch, 50, 128+16)\n",
    "\n",
    "        fused_feat_flat = fused_feat.reshape(-1, fused_feat.size(-1))  # (batch*50, 144)\n",
    "        logits_flat = self.fusion_fc(fused_feat_flat)  # (batch*50, 1)\n",
    "        logits = logits_flat.view(batch_size, -1)  # (batch, 50)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "class EarlyStopping:\n",
    "\n",
    "    def __init__(self, patience=10, min_delta=0.0, verbose=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'  æ—©åœè®¡æ•°å™¨: {self.counter}/{self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        return self.early_stop\n",
    "\n",
    "class TrainingMonitor:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.history = {\n",
    "            'train_loss': [], 'val_loss': [], 'val_acc': [],\n",
    "            'learning_rate': [], 'epoch_time': [], 'train_acc': []\n",
    "        }\n",
    "        self.start_time = time.time()\n",
    "    \n",
    "    def update(self, train_loss, val_loss, val_acc, lr, epoch_time, train_acc=None):\n",
    "        self.history['train_loss'].append(train_loss)\n",
    "        self.history['val_loss'].append(val_loss)\n",
    "        self.history['val_acc'].append(val_acc)\n",
    "        self.history['learning_rate'].append(lr)\n",
    "        self.history['epoch_time'].append(epoch_time)\n",
    "        if train_acc is not None:\n",
    "            self.history['train_acc'].append(train_acc)\n",
    "    \n",
    "    def print_epoch_summary(self, epoch, total_epochs, train_loss, val_loss, val_acc, lr, epoch_time):\n",
    "\n",
    "        elapsed = time.time() - self.start_time\n",
    "        eta = elapsed / epoch * (total_epochs - epoch) if epoch > 0 else 0\n",
    "        \n",
    "        print(f'  Epoch {epoch:3d}/{total_epochs} | '\n",
    "              f'æ—¶é—´: {epoch_time:5.1f}s | '\n",
    "              f'LR: {lr:.6f} | '\n",
    "              f'è®­ç»ƒæŸå¤±: {train_loss:.4f} | '\n",
    "              f'éªŒè¯æŸå¤±: {val_loss:.4f} | '\n",
    "              f'éªŒè¯å‡†ç¡®ç‡: {val_acc:.3f} | '\n",
    "              f'æ€»è€—æ—¶: {elapsed/60:.1f}m | '\n",
    "              f'ETA: {eta/60:.1f}m')\n",
    "\n",
    "def compute_accuracy(predictions, targets, threshold=0.5):\n",
    "\n",
    "    probs = torch.sigmoid(predictions)\n",
    "    binary_preds = (probs > threshold).float()\n",
    "    correct = (binary_preds == targets).float()\n",
    "    return correct.mean().item()\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device, config):\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for batch_idx, (batch_x, batch_y, _) in enumerate(loader):\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        batch_size = batch_x.size(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(batch_x)  \n",
    "        loss = criterion(predictions, batch_y)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        if config.USE_GRADIENT_CLIP:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.GRADIENT_CLIP_VAL)\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * batch_size\n",
    "        total_acc += compute_accuracy(predictions, batch_y) * batch_size\n",
    "        total_samples += batch_size\n",
    "\n",
    "        if (batch_idx + 1) % 20 == 0:\n",
    "            avg_loss = total_loss / total_samples\n",
    "            avg_acc = total_acc / total_samples\n",
    "            print(f'    æ‰¹æ¬¡ {batch_idx+1:03d}/{len(loader)} | '\n",
    "                  f'æŸå¤±: {avg_loss:.4f} | '\n",
    "                  f'å‡†ç¡®ç‡: {avg_acc:.3f}')\n",
    "    \n",
    "    avg_loss = total_loss / total_samples\n",
    "    avg_acc = total_acc / total_samples\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    total_samples = 0\n",
    "    \n",
    "    all_preds, all_targets = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y, _ in loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            batch_size = batch_x.size(0)\n",
    "            \n",
    "            predictions = model(batch_x)  \n",
    "            loss = criterion(predictions, batch_y)\n",
    "            \n",
    "            total_loss += loss.item() * batch_size\n",
    "            total_acc += compute_accuracy(predictions, batch_y) * batch_size\n",
    "            total_samples += batch_size\n",
    "            \n",
    "            all_preds.append(predictions.cpu())\n",
    "            all_targets.append(batch_y.cpu())\n",
    "    \n",
    "    avg_loss = total_loss / total_samples\n",
    "    avg_acc = total_acc / total_samples\n",
    "\n",
    "    all_preds = torch.cat(all_preds, dim=0)\n",
    "    all_targets = torch.cat(all_targets, dim=0)\n",
    "    \n",
    "    return avg_loss, avg_acc, all_preds, all_targets\n",
    "\n",
    "def main():\n",
    "    config = MediumConfig()\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"é˜¶æ®µ1: åŠ è½½æ•°æ®é›†\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    train_dataset = SmartBeamDataset(\n",
    "        config.DATA_DIRS['train'], \n",
    "        max_samples=config.TRAIN_SAMPLES,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    val_dataset = SmartBeamDataset(\n",
    "        config.DATA_DIRS['val'], \n",
    "        max_samples=config.VAL_SAMPLES,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=config.NUM_WORKERS,\n",
    "        pin_memory=config.PIN_MEMORY\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=config.NUM_WORKERS,\n",
    "        pin_memory=config.PIN_MEMORY\n",
    "    )\n",
    "    \n",
    "    print(f\"\\næ•°æ®ç»Ÿè®¡:\")\n",
    "    print(f\"   è®­ç»ƒé›†: {len(train_dataset):,} æ ·æœ¬, {len(train_loader)} æ‰¹æ¬¡\")\n",
    "    print(f\"   éªŒè¯é›†: {len(val_dataset):,} æ ·æœ¬, {len(val_loader)} æ‰¹æ¬¡\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"é˜¶æ®µ2: åˆå§‹åŒ–1D CNNæ¨¡å‹\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    model = BeamDamage1DCNN(\n",
    "        n_freq=5,\n",
    "        n_points=50,\n",
    "        dropout_rate=config.DROPOUT_RATE\n",
    "    ).to(config.DEVICE)\n",
    "    \n",
    "    # è®¡ç®—å‚æ•°é‡\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\" æ¨¡å‹å‚æ•°ç»Ÿè®¡:\")\n",
    "    print(f\"   æ€»å‚æ•°: {total_params:,}\")\n",
    "    print(f\"   å¯è®­ç»ƒå‚æ•°: {trainable_params:,}\")\n",
    "    print(f\"   æ¨¡å‹å¤§å°: {total_params * 4 / 1024**2:.2f} MB (float32)\")\n",
    "    print(f\"   CNNæ¶æ„: 5â†’[32â†’64â†’128]â†’èåˆâ†’è¾“å‡º\")\n",
    "    \n",
    "    pos_weight = torch.tensor([5.0]).to(config.DEVICE)  \n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    \n",
    "    if config.OPTIMIZER == 'AdamW':\n",
    "        optimizer = optim.AdamW(\n",
    "            model.parameters(), \n",
    "            lr=config.LEARNING_RATE, \n",
    "            weight_decay=config.WEIGHT_DECAY\n",
    "        )\n",
    "    else:\n",
    "        optimizer = optim.Adam(\n",
    "            model.parameters(), \n",
    "            lr=config.LEARNING_RATE\n",
    "        )\n",
    "\n",
    "    if config.LR_SCHEDULER == 'cosine':\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, T_max=config.EPOCHS, eta_min=1e-6)\n",
    "    else:\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=config.LR_FACTOR, \n",
    "            patience=config.LR_PATIENCE, verbose=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(\n",
    "        patience=config.EARLY_STOP_PATIENCE,\n",
    "        min_delta=config.EARLY_STOP_MIN_DELTA,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    monitor = TrainingMonitor()\n",
    "    best_val_acc = 0.0\n",
    "    best_epoch = 0\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"é˜¶æ®µ3: å¼€å§‹CNNè®­ç»ƒ\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for epoch in range(1, config.EPOCHS + 1):\n",
    "        epoch_start = time.time()\n",
    "\n",
    "        train_loss, train_acc = train_epoch(\n",
    "            model, train_loader, criterion, optimizer, config.DEVICE, config\n",
    "        )\n",
    "\n",
    "        val_loss, val_acc, val_preds, val_targets = validate(\n",
    "            model, val_loader, criterion, config.DEVICE\n",
    "        )\n",
    "\n",
    "        if config.LR_SCHEDULER == 'reduce_on_plateau':\n",
    "            scheduler.step(val_loss)\n",
    "        else:\n",
    "            scheduler.step()\n",
    "        \n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        epoch_time = time.time() - epoch_start\n",
    "\n",
    "        monitor.update(train_loss, val_loss, val_acc, current_lr, epoch_time, train_acc)\n",
    "\n",
    "        monitor.print_epoch_summary(\n",
    "            epoch, config.EPOCHS, train_loss, val_loss, val_acc, \n",
    "            current_lr, epoch_time\n",
    "        )\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_epoch = epoch\n",
    "            \n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'train_acc': train_acc,\n",
    "                'val_loss': val_loss,\n",
    "                'val_acc': val_acc,\n",
    "                'config': config.__dict__,\n",
    "                'history': monitor.history\n",
    "            }, config.SAVE_DIR / 'best_model.pth')\n",
    "            \n",
    "            print(f\"  ä¿å­˜æœ€ä½³æ¨¡å‹ @ Epoch {epoch}: å‡†ç¡®ç‡ = {val_acc:.3f}\")\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            checkpoint_path = config.SAVE_DIR / f'checkpoint_epoch_{epoch}.pth'\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_acc': val_acc,\n",
    "            }, checkpoint_path)\n",
    "            print(f\"  æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}\")\n",
    "\n",
    "        if early_stopping(val_loss):\n",
    "            print(f\"\\n  æ—©åœè§¦å‘äºç¬¬ {epoch} å‘¨æœŸ\")\n",
    "            print(f\"   æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.3%} @ Epoch {best_epoch}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"è®­ç»ƒå®Œæˆ!\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    final_model_path = config.SAVE_DIR / 'final_model.pth'\n",
    "    torch.save(model.state_dict(), final_model_path)\n",
    "\n",
    "    history_path = config.SAVE_DIR / 'training_history.npy'\n",
    "    np.save(history_path, monitor.history)\n",
    "\n",
    "    config_dict = {k: v for k, v in config.__dict__.items() \n",
    "                  if not k.startswith('_') and not callable(v)}\n",
    "    \n",
    "    with open(config.SAVE_DIR / 'training_config.json', 'w') as f:\n",
    "        json.dump(config_dict, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"\\n è®­ç»ƒæ€»ç»“:\")\n",
    "    print(f\"   æ€»è®­ç»ƒå‘¨æœŸ: {len(monitor.history['train_loss'])}\")\n",
    "    print(f\"   æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.3%} @ Epoch {best_epoch}\")\n",
    "    print(f\"   æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {monitor.history['val_acc'][-1]:.3%}\")\n",
    "    print(f\"   è®­ç»ƒ/éªŒè¯æŸå¤±æ¯”: {monitor.history['train_loss'][-1]/monitor.history['val_loss'][-1]:.2f}\")\n",
    "\n",
    "    print(f\"\\n ä¿å­˜çš„æ–‡ä»¶:\")\n",
    "    print(f\"   æœ€ä½³æ¨¡å‹: {config.SAVE_DIR / 'best_model.pth'}\")\n",
    "    print(f\"   æœ€ç»ˆæ¨¡å‹: {final_model_path}\")\n",
    "    print(f\"   è®­ç»ƒå†å²: {history_path}\")\n",
    "    print(f\"   è®­ç»ƒé…ç½®: {config.SAVE_DIR / 'training_config.json'}\")\n",
    "\n",
    "    plot_training_curves(monitor.history, config.SAVE_DIR)\n",
    "    \n",
    "    return model, monitor.history, best_val_acc\n",
    "\n",
    "def plot_training_curves(history, save_dir):\n",
    "\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    fig.suptitle('1D CNN Training Results (10k Train + 4k Val)', fontsize=16, y=1.02)\n",
    "\n",
    "    ax = axes[0, 0]\n",
    "    ax.plot(epochs, history['train_loss'], 'b-', linewidth=2, label='Training Loss')\n",
    "    ax.plot(epochs, history['val_loss'], 'r-', linewidth=2, label='Validation Loss')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title('Training & Validation Loss')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    if max(history['train_loss']) > 10 * max(history['val_loss']):\n",
    "        ax.set_yscale('log')\n",
    "\n",
    "    ax = axes[0, 1]\n",
    "    ax.plot(epochs, history['val_acc'], 'g-', linewidth=2, label='Validation Acc')\n",
    "    if 'train_acc' in history and len(history['train_acc']) == len(epochs):\n",
    "        ax.plot(epochs, history['train_acc'], 'b--', linewidth=1.5, alpha=0.7, label='Training Acc')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title('Training & Validation Accuracy')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim([0.5, 1.0])\n",
    "\n",
    "    ax = axes[0, 2]\n",
    "    ax.plot(epochs, history['learning_rate'], 'm-', linewidth=2)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Learning Rate')\n",
    "    ax.set_title('Learning Rate Schedule')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_yscale('log')\n",
    "\n",
    "    ax = axes[1, 0]\n",
    "    ax.plot(epochs, history['epoch_time'], 'c-', linewidth=2)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Time (seconds)')\n",
    "    ax.set_title('Epoch Duration')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.axhline(y=np.mean(history['epoch_time']), color='r', linestyle='--', alpha=0.5, \n",
    "               label=f'Mean: {np.mean(history[\"epoch_time\"]):.1f}s')\n",
    "    ax.legend()\n",
    "\n",
    "    ax = axes[1, 1]\n",
    "    scatter = ax.scatter(history['val_loss'], history['val_acc'], \n",
    "                        c=range(len(history['val_loss'])), cmap='viridis', \n",
    "                        s=30, alpha=0.7)\n",
    "    ax.set_xlabel('Validation Loss')\n",
    "    ax.set_ylabel('Validation Accuracy')\n",
    "    ax.set_title('Loss vs Accuracy Correlation')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.colorbar(scatter, ax=ax, label='Epoch')\n",
    "\n",
    "    ax = axes[1, 2]\n",
    "    early_epochs = min(30, len(epochs))\n",
    "    ax.plot(epochs[:early_epochs], history['train_loss'][:early_epochs], 'b-o', \n",
    "            linewidth=1.5, markersize=3, label='Training Loss')\n",
    "    ax.plot(epochs[:early_epochs], history['val_loss'][:early_epochs], 'r-s', \n",
    "            linewidth=1.5, markersize=3, label='Validation Loss')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title(f'Early Training (First {early_epochs} Epochs)')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plot_path = save_dir / 'training_curves.png'\n",
    "    plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n è®­ç»ƒæ›²çº¿å·²ä¿å­˜: {plot_path}\")\n",
    "\n",
    "def quick_system_check():\n",
    "    \"\"\"å¿«é€Ÿç³»ç»Ÿæ£€æŸ¥\"\"\"\n",
    "    print(\" ç³»ç»Ÿæ£€æŸ¥å¼€å§‹...\")\n",
    "    \n",
    "    checks_passed = True\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        print(f\" GPU: {gpu_name} ({gpu_memory:.1f} GB)\")\n",
    "    else:\n",
    "        print(\"  æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒï¼ˆä¼šå¾ˆæ…¢ï¼‰\")\n",
    "\n",
    "    data_dirs = ['./dataset/train', './dataset/val', './dataset/test']\n",
    "    for dir_path in data_dirs:\n",
    "        path = Path(dir_path)\n",
    "        if path.exists():\n",
    "            pkl_files = list(path.glob('batch_*.pkl'))\n",
    "            print(f\" {dir_path}: å‘ç° {len(pkl_files)} ä¸ªæ‰¹æ¬¡æ–‡ä»¶\")\n",
    "        else:\n",
    "            print(f\" {dir_path}: ç›®å½•ä¸å­˜åœ¨\")\n",
    "            checks_passed = False\n",
    "    \n",
    "    return checks_passed\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 70)\n",
    "    print(\"æ¢ç»“æ„æŸä¼¤è¯†åˆ« - 1D CNNä¸­ç­‰è§„æ¨¡è®­ç»ƒ\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    if not quick_system_check():\n",
    "        print(\"\\nç³»ç»Ÿæ£€æŸ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä»¥ä¸Šé—®é¢˜\")\n",
    "        exit(1)\n",
    "    \n",
    "    print(\"\\nç³»ç»Ÿæ£€æŸ¥é€šè¿‡ï¼Œå¼€å§‹è®­ç»ƒæµç¨‹\")\n",
    "    \n",
    "    try:\n",
    " \n",
    "        model, history, best_acc = main()\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"ğŸ‰ CNNè®­ç»ƒæˆåŠŸå®Œæˆ!\")\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "        original_fcn_acc = 0.82  # åŸå§‹FCNæ€§èƒ½\n",
    "        improvement = best_acc - original_fcn_acc\n",
    "        print(f\"\\næ€§èƒ½å¯¹æ¯”:\")\n",
    "        print(f\"   åŸå§‹FCN (10kæ•°æ®): {original_fcn_acc:.1%}\")\n",
    "        print(f\"   1D CNN (10kæ•°æ®): {best_acc:.1%}\")\n",
    "        if improvement > 0:\n",
    "            print(f\"   æ€§èƒ½æå‡: +{improvement*100:.1f}%\")\n",
    "        else:\n",
    "            print(f\"   æ€§èƒ½å˜åŒ–: {improvement*100:.1f}%\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nè®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Torch GPU)",
   "language": "python",
   "name": "torch_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
