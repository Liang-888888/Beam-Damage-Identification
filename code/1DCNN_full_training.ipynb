{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4609d2-268f-4df7-a475-da4d95c26232",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import time\n",
    "import json\n",
    "import warnings\n",
    "import gc\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class FullScaleConfig:\n",
    "    \n",
    "    # æ•°æ®è·¯å¾„\n",
    "    DATA_DIRS = {\n",
    "        'train': './dataset/train',    # 90,000æ ·æœ¬\n",
    "        'val': './dataset/val',        # 15,000æ ·æœ¬\n",
    "        'test': './dataset/test'       # 5,000æ ·æœ¬\n",
    "    }\n",
    "    \n",
    "    TRAIN_SAMPLES = None      # å®Œæ•´90k\n",
    "    VAL_SAMPLES = None        # å®Œæ•´15k\n",
    "    TEST_SAMPLES = None       # å®Œæ•´5k\n",
    "    \n",
    "    N_FREQ = 5                \n",
    "    N_POINTS = 50            \n",
    "    CONV_CHANNELS = [32, 64, 128]  \n",
    "    \n",
    "\n",
    "    BATCH_SIZE = 256         \n",
    "    EPOCHS = 200             \n",
    "    LEARNING_RATE = 0.0005    \n",
    "    WEIGHT_DECAY = 1e-4       \n",
    "    DROPOUT_RATE = 0.1       \n",
    "\n",
    "    OPTIMIZER = 'AdamW'\n",
    "    USE_GRADIENT_CLIP = False  \n",
    "    USE_MIXED_PRECISION = True  \n",
    "\n",
    "    LR_SCHEDULER = 'cosine'    \n",
    "    LR_PATIENCE = 10\n",
    "    LR_FACTOR = 0.5\n",
    "    COSINE_T_MAX = 100         \n",
    "\n",
    "    EARLY_STOP_PATIENCE = 35   \n",
    "    EARLY_STOP_MIN_DELTA = 0.0005\n",
    "\n",
    "    NUM_WORKERS = 4            \n",
    "    PIN_MEMORY = True          \n",
    "    PREFETCH_FACTOR = 2        \n",
    "\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    RUN_ID = f\"full_{int(time.time())}\"\n",
    "    SAVE_DIR = Path(f'./results/{RUN_ID}')\n",
    "    SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    def __init__(self):\n",
    "        print(\"=\" * 80)\n",
    "        print(\"å®Œæ•´è§„æ¨¡è®­ç»ƒé…ç½® - 1D CNNæ¶æ„\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"æ•°æ®è§„æ¨¡: å®Œæ•´æ•°æ®é›† (90kè®­ç»ƒ + 15kéªŒè¯ + 5kæµ‹è¯•)\")\n",
    "        print(f\"æ¨¡å‹æ¶æ„: 1D CNN - è¾“å…¥: {self.N_FREQ}é¢‘ç‡ + {self.N_POINTS}ç©ºé—´ç‚¹\")\n",
    "        print(f\"è®­ç»ƒå‚æ•°: LR={self.LEARNING_RATE}, BS={self.BATCH_SIZE}\")\n",
    "        print(f\"æœ€å¤§å‘¨æœŸ: {self.EPOCHS}, æ—©åœè€å¿ƒ: {self.EARLY_STOP_PATIENCE}\")\n",
    "        print(f\"ä½¿ç”¨è®¾å¤‡: {self.DEVICE}\")\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            gpu_name = torch.cuda.get_device_name(0)\n",
    "            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "            print(f\"GPU: {gpu_name} ({gpu_memory:.1f} GB)\")\n",
    "            print(f\"æ··åˆç²¾åº¦è®­ç»ƒ: {self.USE_MIXED_PRECISION}\")\n",
    "        \n",
    "        print(f\"è¾“å‡ºç›®å½•: {self.SAVE_DIR}\")\n",
    "\n",
    "\n",
    "class CachedBeamDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_dir, max_samples=None, device='cpu'):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.max_samples = max_samples\n",
    "        self.device = device\n",
    "        \n",
    "        print(f\"\\nåŠ è½½å¹¶ç¼“å­˜æ•°æ®ä»: {data_dir}\")\n",
    "        print(f\"   ç›®æ ‡æ ·æœ¬æ•°: {max_samples if max_samples else 'å…¨éƒ¨'}\")\n",
    "        \n",
    "        self.x_data = []\n",
    "        self.y_data = []\n",
    "        self.metadata = []\n",
    "        \n",
    "        start_time = time.time()\n",
    "        self._load_and_cache_all_data()\n",
    "        load_time = time.time() - start_time\n",
    "        \n",
    "        self.x_data = torch.stack(self.x_data).to(self.device)\n",
    "        self.y_data = torch.stack(self.y_data).to(self.device)\n",
    "        \n",
    "        print(f\"åŠ è½½å®Œæˆ: {len(self):,} ä¸ªæ ·æœ¬\")\n",
    "        \n",
    "        del self.metadata\n",
    "        gc.collect()\n",
    "    \n",
    "    def _load_and_cache_all_data(self):\n",
    "\n",
    "        batch_files = sorted(self.data_dir.glob('batch_*.pkl'))\n",
    "        \n",
    "        if not batch_files:\n",
    "            raise FileNotFoundError(f\"æœªæ‰¾åˆ°æ‰¹æ¬¡æ–‡ä»¶: {self.data_dir}\")\n",
    "        \n",
    "        print(f\"   å‘ç° {len(batch_files)} ä¸ªæ‰¹æ¬¡æ–‡ä»¶\")\n",
    "        \n",
    "        loaded_samples = 0\n",
    "        for file_idx, batch_file in enumerate(batch_files):\n",
    "            if self.max_samples and loaded_samples >= self.max_samples:\n",
    "                break\n",
    "            \n",
    "            with open(batch_file, 'rb') as f:\n",
    "                batch_data = pickle.load(f)\n",
    "            \n",
    "            remaining = self.max_samples - loaded_samples if self.max_samples else len(batch_data)\n",
    "            samples_to_add = min(len(batch_data), remaining)\n",
    "            \n",
    "            for i in range(samples_to_add):\n",
    "                sample = batch_data[i]\n",
    "                \n",
    "                freq = torch.FloatTensor(sample['input']['frequencies'][:5])\n",
    "                mode_shapes = torch.FloatTensor(sample['input']['mode_shapes'][:5])\n",
    "                x = torch.cat([freq, mode_shapes.flatten()])\n",
    "                \n",
    "                y = torch.FloatTensor(sample['output']['binary_labels'])\n",
    "                \n",
    "                self.x_data.append(x)\n",
    "                self.y_data.append(y)\n",
    "                loaded_samples += 1\n",
    "            \n",
    "            if (file_idx + 1) % 5 == 0 or (file_idx + 1) == len(batch_files):\n",
    "                print(f\"   [{file_idx+1:03d}/{len(batch_files)}] å·²åŠ è½½ {loaded_samples:,} æ ·æœ¬\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x_data[idx], self.y_data[idx]\n",
    "    \n",
    "    def get_stats(self):\n",
    "        if len(self) == 0:\n",
    "            return {}\n",
    "        \n",
    "        damage_ratio = self.y_data.mean().item()\n",
    "        \n",
    "        x_mean = self.x_data.mean().item()\n",
    "        x_std = self.x_data.std().item()\n",
    "        \n",
    "        return {\n",
    "            'n_samples': len(self),\n",
    "            'damage_ratio': damage_ratio,\n",
    "            'x_mean': x_mean,\n",
    "            'x_std': x_std,\n",
    "            'x_shape': tuple(self.x_data.shape),\n",
    "            'y_shape': tuple(self.y_data.shape)\n",
    "        }\n",
    "\n",
    "class EnhancedBeamCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_freq=5, n_points=50, channels=[32, 64, 128], dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.spatial_encoder = nn.Sequential(\n",
    "            nn.Conv1d(n_freq, channels[0], kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(channels[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            nn.Conv1d(channels[0], channels[1], kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(channels[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),  \n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            nn.Conv1d(channels[1], channels[2], kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(channels[2]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate * 0.5),\n",
    "        )\n",
    "        \n",
    "        self.freq_encoder = nn.Sequential(\n",
    "            nn.Linear(n_freq, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "\n",
    "            nn.Upsample(scale_factor=2, mode='linear'),\n",
    "\n",
    "            nn.Conv1d(channels[2] + 16, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate * 0.3),\n",
    "            \n",
    "            nn.Conv1d(64, 1, kernel_size=1), \n",
    "        )\n",
    "        \n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        frequencies = x[:, :5]  # [batch, 5]\n",
    "        mode_shapes = x[:, 5:].view(batch_size, 5, -1)  # [batch, 5, 50]\n",
    "        \n",
    "        spatial_feat = self.spatial_encoder(mode_shapes)  # [batch, 128, 25]\n",
    "        \n",
    "        freq_feat = self.freq_encoder(frequencies)  # [batch, 16]\n",
    "        freq_feat_expanded = freq_feat.unsqueeze(-1)  # [batch, 16, 1]\n",
    "        freq_feat_expanded = freq_feat_expanded.expand(-1, -1, spatial_feat.size(-1))  # [batch, 16, 25]\n",
    "        \n",
    "        fused_feat = torch.cat([spatial_feat, freq_feat_expanded], dim=1)  # [batch, 144, 25]\n",
    "        \n",
    "        output = self.decoder(fused_feat)  # [batch, 1, 50]\n",
    "        output = output.squeeze(1)  # [batch, 50]\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "class TrainingMonitor:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.history = {\n",
    "            'epoch': [],\n",
    "            'train_loss': [], 'train_acc': [],\n",
    "            'val_loss': [], 'val_acc': [],\n",
    "            'learning_rate': [], 'epoch_time': [],\n",
    "            'grad_norm': []\n",
    "        }\n",
    "        self.start_time = time.time()\n",
    "        self.best_val_acc = 0.0\n",
    "        self.best_epoch = 0\n",
    "        self.best_model_state = None\n",
    "    \n",
    "    def update(self, epoch, train_loss, train_acc, val_loss, val_acc, lr, epoch_time, grad_norm=None):\n",
    "\n",
    "        self.history['epoch'].append(epoch)\n",
    "        self.history['train_loss'].append(train_loss)\n",
    "        self.history['train_acc'].append(train_acc)\n",
    "        self.history['val_loss'].append(val_loss)\n",
    "        self.history['val_acc'].append(val_acc)\n",
    "        self.history['learning_rate'].append(lr)\n",
    "        self.history['epoch_time'].append(epoch_time)\n",
    "        \n",
    "        if grad_norm is not None:\n",
    "            self.history['grad_norm'].append(grad_norm)\n",
    "        \n",
    "        if val_acc > self.best_val_acc:\n",
    "            self.best_val_acc = val_acc\n",
    "            self.best_epoch = epoch\n",
    "    \n",
    "    def print_epoch_summary(self, epoch, total_epochs, train_loss, val_loss, val_acc, lr, epoch_time):\n",
    "        elapsed = time.time() - self.start_time\n",
    "        elapsed_min = elapsed / 60\n",
    "        eta_min = elapsed_min / epoch * (total_epochs - epoch) if epoch > 0 else 0\n",
    "        \n",
    "        print(f\"Epoch {epoch:3d}/{total_epochs} | \"\n",
    "              f\"æ—¶é—´: {epoch_time:5.1f}s | \"\n",
    "              f\"LR: {lr:.2e} | \"\n",
    "              f\"è®­ç»ƒæŸå¤±: {train_loss:.4f} | \"\n",
    "              f\"éªŒè¯æŸå¤±: {val_loss:.4f} | \"\n",
    "              f\"éªŒè¯å‡†ç¡®ç‡: {val_acc:.4f} | \"\n",
    "              f\"å·²è®­ç»ƒ: {elapsed_min:.1f}m | \"\n",
    "              f\"ETA: {eta_min:.1f}m\")\n",
    "    \n",
    "    def get_best_results(self):\n",
    "        if len(self.history['epoch']) == 0:\n",
    "            return None\n",
    "        \n",
    "        return {\n",
    "            'best_epoch': self.best_epoch,\n",
    "            'best_val_acc': self.best_val_acc,\n",
    "            'best_val_loss': min(self.history['val_loss']) if self.history['val_loss'] else float('inf'),\n",
    "            'final_train_acc': self.history['train_acc'][-1] if self.history['train_acc'] else 0,\n",
    "            'final_val_acc': self.history['val_acc'][-1] if self.history['val_acc'] else 0\n",
    "        }\n",
    "\n",
    "class EarlyStopping:\n",
    "\n",
    "    def __init__(self, patience=10, min_delta=0.0, verbose=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.early_stop = False\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def __call__(self, val_loss):\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            improved = True\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            improved = False\n",
    "            \n",
    "            if self.verbose and self.counter > 0:\n",
    "                print(f\"   æ—©åœè®¡æ•°å™¨: {self.counter}/{self.patience}\")\n",
    "            \n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        \n",
    "        return self.early_stop, improved\n",
    "\n",
    "\n",
    "def compute_metrics(predictions, targets, threshold=0.5):\n",
    "\n",
    "    probs = torch.sigmoid(predictions)\n",
    "    binary_preds = (probs > threshold).float()\n",
    "\n",
    "    accuracy = (binary_preds == targets).float().mean().item()\n",
    "    \n",
    "    damage_mask = (targets == 1)\n",
    "    if damage_mask.any():\n",
    "        damage_recall = (binary_preds[damage_mask] == 1).float().mean().item()\n",
    "        damage_precision = (binary_preds[targets == 1] == 1).float().mean().item() if (targets == 1).any() else 0\n",
    "    else:\n",
    "        damage_recall = 0\n",
    "        damage_precision = 0\n",
    "\n",
    "    if damage_precision + damage_recall > 0:\n",
    "        damage_f1 = 2 * damage_precision * damage_recall / (damage_precision + damage_recall)\n",
    "    else:\n",
    "        damage_f1 = 0\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'damage_recall': damage_recall,\n",
    "        'damage_precision': damage_precision,\n",
    "        'damage_f1': damage_f1,\n",
    "        'probs': probs,\n",
    "        'preds': binary_preds\n",
    "    }\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device, config, scaler=None):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    for batch_idx, (batch_x, batch_y) in enumerate(dataloader):\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        batch_size = batch_x.size(0)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=config.USE_MIXED_PRECISION and device.type == 'cuda'):\n",
    "            predictions = model(batch_x)\n",
    "            loss = criterion(predictions, batch_y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if scaler is not None:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * batch_size\n",
    "        total_samples += batch_size\n",
    "\n",
    "        all_predictions.append(predictions.detach())\n",
    "        all_targets.append(batch_y.detach())\n",
    "\n",
    "        if (batch_idx + 1) % 50 == 0:\n",
    "            avg_loss = total_loss / total_samples\n",
    "            print(f'    æ‰¹æ¬¡ {batch_idx+1:04d}/{len(dataloader)} | å¹³å‡æŸå¤±: {avg_loss:.4f}')\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    all_predictions = torch.cat(all_predictions, dim=0)\n",
    "    all_targets = torch.cat(all_targets, dim=0)\n",
    "    metrics = compute_metrics(all_predictions, all_targets)\n",
    "    \n",
    "    return avg_loss, metrics['accuracy']\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, dataloader, criterion, device):\n",
    "\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    for batch_x, batch_y in dataloader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        batch_size = batch_x.size(0)\n",
    "        \n",
    "        predictions = model(batch_x)\n",
    "        loss = criterion(predictions, batch_y)\n",
    "        \n",
    "        total_loss += loss.item() * batch_size\n",
    "        total_samples += batch_size\n",
    "        \n",
    "        all_predictions.append(predictions)\n",
    "        all_targets.append(batch_y)\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    all_predictions = torch.cat(all_predictions, dim=0)\n",
    "    all_targets = torch.cat(all_targets, dim=0)\n",
    "    metrics = compute_metrics(all_predictions, all_targets)\n",
    "    \n",
    "    return avg_loss, metrics['accuracy'], metrics\n",
    "\n",
    "def train_full_scale():\n",
    "    config = FullScaleConfig()\n",
    "    \n",
    "    config_dict = {k: v for k, v in config.__dict__.items() \n",
    "                  if not k.startswith('_') and not callable(v)}\n",
    "    with open(config.SAVE_DIR / 'config.json', 'w') as f:\n",
    "        json.dump(config_dict, f, indent=2, default=str)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"é˜¶æ®µ1: åŠ è½½æ•°æ®é›†\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    train_dataset = CachedBeamDataset(\n",
    "        config.DATA_DIRS['train'],\n",
    "        max_samples=config.TRAIN_SAMPLES,\n",
    "        device=config.DEVICE\n",
    "    )\n",
    "    \n",
    "    val_dataset = CachedBeamDataset(\n",
    "        config.DATA_DIRS['val'],\n",
    "        max_samples=config.VAL_SAMPLES,\n",
    "        device=config.DEVICE\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=0,  \n",
    "        pin_memory=False\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config.BATCH_SIZE * 2,  \n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=False\n",
    "    )\n",
    "    \n",
    "    print(f\"\\næ•°æ®é›†ç»Ÿè®¡:\")\n",
    "    print(f\"  è®­ç»ƒé›†: {len(train_dataset):,} æ ·æœ¬, {len(train_loader)} æ‰¹æ¬¡\")\n",
    "    print(f\"  éªŒè¯é›†: {len(val_dataset):,} æ ·æœ¬, {len(val_loader)} æ‰¹æ¬¡\")\n",
    "    \n",
    "    train_stats = train_dataset.get_stats()\n",
    "    val_stats = val_dataset.get_stats()\n",
    "    print(f\"\\n è®­ç»ƒé›†ç»Ÿè®¡: æŸä¼¤æ¯”ä¾‹={train_stats['damage_ratio']:.3f}\")\n",
    "    print(f\" éªŒè¯é›†ç»Ÿè®¡: æŸä¼¤æ¯”ä¾‹={val_stats['damage_ratio']:.3f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"é˜¶æ®µ2: åˆå§‹åŒ–æ¨¡å‹\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    model = EnhancedBeamCNN(\n",
    "        n_freq=config.N_FREQ,\n",
    "        n_points=config.N_POINTS,\n",
    "        channels=config.CONV_CHANNELS,\n",
    "        dropout_rate=config.DROPOUT_RATE\n",
    "    ).to(config.DEVICE)\n",
    "    \n",
    "    total_params = model.count_parameters()\n",
    "    print(f\" æ¨¡å‹å‚æ•°é‡: {total_params:,}\")\n",
    "    print(f\"   æ¨¡å‹å¤§å°: {total_params * 4 / 1024**2:.2f} MB (float32)\")\n",
    "    \n",
    "    pos_weight = torch.tensor([5.0]).to(config.DEVICE)  \n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    \n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config.LEARNING_RATE,\n",
    "        weight_decay=config.WEIGHT_DECAY,\n",
    "        betas=(0.9, 0.999)\n",
    "    )\n",
    "    \n",
    "\n",
    "    if config.LR_SCHEDULER == 'cosine':\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, T_max=config.COSINE_T_MAX, eta_min=1e-6)\n",
    "    else:\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=config.LR_FACTOR,\n",
    "            patience=config.LR_PATIENCE, verbose=True)\n",
    "    \n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler() if config.USE_MIXED_PRECISION and config.DEVICE.type == 'cuda' else None\n",
    "    \n",
    "\n",
    "    early_stopping = EarlyStopping(\n",
    "        patience=config.EARLY_STOP_PATIENCE,\n",
    "        min_delta=config.EARLY_STOP_MIN_DELTA,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "\n",
    "    monitor = TrainingMonitor()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"é˜¶æ®µ3: å¼€å§‹å®Œæ•´è§„æ¨¡è®­ç»ƒ\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"å¼€å§‹æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"ä½¿ç”¨æ··åˆç²¾åº¦: {scaler is not None}\")\n",
    "    \n",
    "\n",
    "    for epoch in range(1, config.EPOCHS + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch:03d}/{config.EPOCHS}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        print(\"è®­ç»ƒé˜¶æ®µ:\")\n",
    "        train_loss, train_acc = train_epoch(\n",
    "            model, train_loader, criterion, optimizer, config.DEVICE, config, scaler\n",
    "        )\n",
    "        \n",
    "        print(\"éªŒè¯é˜¶æ®µ:\")\n",
    "        val_loss, val_acc, val_metrics = validate(\n",
    "            model, val_loader, criterion, config.DEVICE\n",
    "        )\n",
    "\n",
    "        if config.LR_SCHEDULER == 'reduce_on_plateau':\n",
    "            scheduler.step(val_loss)\n",
    "        else:\n",
    "            scheduler.step()\n",
    "        \n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "\n",
    "        monitor.update(epoch, train_loss, train_acc, val_loss, val_acc, current_lr, epoch_time)\n",
    "\n",
    "        monitor.print_epoch_summary(\n",
    "            epoch, config.EPOCHS, train_loss, val_loss, val_acc, current_lr, epoch_time\n",
    "        )\n",
    "\n",
    "        print(f\"    éªŒè¯è¯¦ç»†æŒ‡æ ‡: å‡†ç¡®ç‡={val_acc:.4f}, å¬å›ç‡={val_metrics['damage_recall']:.4f}, \"\n",
    "              f\"F1={val_metrics['damage_f1']:.4f}\")\n",
    "\n",
    "        early_stop, improved = early_stopping(val_loss)\n",
    "\n",
    "        if improved:\n",
    "            best_checkpoint = {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'train_acc': train_acc,\n",
    "                'val_loss': val_loss,\n",
    "                'val_acc': val_acc,\n",
    "                'val_metrics': val_metrics,\n",
    "                'config': config_dict\n",
    "            }\n",
    "            \n",
    "            torch.save(best_checkpoint, config.SAVE_DIR / 'best_model.pth')\n",
    "            print(f\"   ä¿å­˜æœ€ä½³æ¨¡å‹ @ Epoch {epoch}: éªŒè¯å‡†ç¡®ç‡ = {val_acc:.4f}\")\n",
    "\n",
    "        if epoch % 10 == 0 or epoch == config.EPOCHS:\n",
    "            checkpoint_path = config.SAVE_DIR / f'checkpoint_epoch_{epoch:03d}.pth'\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'val_acc': val_acc,\n",
    "                'val_loss': val_loss\n",
    "            }, checkpoint_path)\n",
    "            print(f\"   æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}\")\n",
    "\n",
    "        if early_stop:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(\"è®­ç»ƒå®Œæˆ - æ—©åœè§¦å‘\")\n",
    "            print(f\"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {monitor.best_val_acc:.4f} @ Epoch {monitor.best_epoch}\")\n",
    "            break\n",
    "\n",
    "    if not early_stopping.early_stop:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"è®­ç»ƒå®Œæˆ - è¾¾åˆ°æœ€å¤§å‘¨æœŸæ•°\")\n",
    "        print(f\"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {monitor.best_val_acc:.4f} @ Epoch {monitor.best_epoch}\")\n",
    "    \n",
    "    total_training_time = time.time() - monitor.start_time\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"é˜¶æ®µ4: ä¿å­˜æœ€ç»ˆç»“æœ\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    torch.save(model.state_dict(), config.SAVE_DIR / 'final_model.pth')\n",
    "\n",
    "    np.save(config.SAVE_DIR / 'training_history.npy', monitor.history)\n",
    "\n",
    "    best_results = monitor.get_best_results()\n",
    "    if best_results:\n",
    "        training_summary = {\n",
    "            'total_epochs_trained': len(monitor.history['epoch']),\n",
    "            'total_training_time_minutes': total_training_time / 60,\n",
    "            'final_train_acc': best_results['final_train_acc'],\n",
    "            'final_val_acc': best_results['final_val_acc'],\n",
    "            'best_epoch': best_results['best_epoch'],\n",
    "            'best_val_acc': best_results['best_val_acc'],\n",
    "            'best_val_loss': best_results['best_val_loss'],\n",
    "            'config': config_dict\n",
    "        }\n",
    "        \n",
    "        with open(config.SAVE_DIR / 'training_summary.json', 'w') as f:\n",
    "            json.dump(training_summary, f, indent=2, default=str)\n",
    "\n",
    "    plot_training_curves(monitor.history, config.SAVE_DIR)\n",
    "    \n",
    "    print(f\"\\n è®­ç»ƒå®Œæˆ!\")\n",
    "    print(f\" æ‰€æœ‰ç»“æœä¿å­˜åœ¨: {config.SAVE_DIR.absolute()}\")\n",
    "    print(f\"  æ€»è®­ç»ƒæ—¶é—´: {total_training_time/60:.1f}åˆ†é’Ÿ\")\n",
    "    print(f\" æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {monitor.best_val_acc:.4f} @ Epoch {monitor.best_epoch}\")\n",
    "    \n",
    "    return model, monitor.history, best_results\n",
    "\n",
    "def evaluate_on_testset(model_path, config_class=FullScaleConfig):\n",
    "\n",
    "    config = config_class()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"æœ€ç»ˆé˜¶æ®µ: åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    test_dataset = CachedBeamDataset(\n",
    "        config.DATA_DIRS['test'],\n",
    "        max_samples=config.TEST_SAMPLES,\n",
    "        device=config.DEVICE\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=config.BATCH_SIZE * 2,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=False\n",
    "    )\n",
    "    \n",
    "    print(f\" æµ‹è¯•é›†: {len(test_dataset):,} æ ·æœ¬\")\n",
    "\n",
    "    checkpoint = torch.load(model_path, map_location=config.DEVICE)\n",
    "    model = EnhancedBeamCNN(\n",
    "        n_freq=config.N_FREQ,\n",
    "        n_points=config.N_POINTS,\n",
    "        channels=config.CONV_CHANNELS,\n",
    "        dropout_rate=config.DROPOUT_RATE\n",
    "    ).to(config.DEVICE)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\" åŠ è½½æ¨¡å‹: {model_path}\")\n",
    "    print(f\"   æ¥è‡ªEpoch {checkpoint.get('epoch', 'unknown')}\")\n",
    "\n",
    "    pos_weight = torch.tensor([5.0]).to(config.DEVICE)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    \n",
    "    test_loss, test_acc, test_metrics = validate(model, test_loader, criterion, config.DEVICE)\n",
    "    \n",
    "    print(f\"\\n æµ‹è¯•é›†ç»“æœ:\")\n",
    "    print(f\"   æµ‹è¯•æŸå¤±: {test_loss:.4f}\")\n",
    "    print(f\"   æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.4f}\")\n",
    "    print(f\"   æŸä¼¤å¬å›ç‡: {test_metrics['damage_recall']:.4f}\")\n",
    "    print(f\"   æŸä¼¤ç²¾ç¡®ç‡: {test_metrics['damage_precision']:.4f}\")\n",
    "    print(f\"   æŸä¼¤F1åˆ†æ•°: {test_metrics['damage_f1']:.4f}\")\n",
    "\n",
    "    test_results = {\n",
    "        'test_loss': test_loss,\n",
    "        'test_accuracy': test_acc,\n",
    "        'test_damage_recall': test_metrics['damage_recall'],\n",
    "        'test_damage_precision': test_metrics['damage_precision'],\n",
    "        'test_damage_f1': test_metrics['damage_f1'],\n",
    "        'model_epoch': checkpoint.get('epoch', 'unknown'),\n",
    "        'evaluation_time': time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }\n",
    "    \n",
    "    with open(config.SAVE_DIR / 'test_results.json', 'w') as f:\n",
    "        json.dump(test_results, f, indent=2)\n",
    "\n",
    "    visualize_test_predictions(model, test_dataset, config, num_samples=5)\n",
    "    \n",
    "    return test_results\n",
    "\n",
    "def visualize_test_predictions(model, test_dataset, config, num_samples=5):\n",
    "    model.eval()\n",
    "    \n",
    "\n",
    "    indices = np.random.choice(len(test_dataset), min(num_samples, len(test_dataset)), replace=False)\n",
    "    \n",
    "    fig, axes = plt.subplots(num_samples, 2, figsize=(12, 3 * num_samples))\n",
    "    if num_samples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        x, y_true = test_dataset[idx]\n",
    "        x = x.unsqueeze(0).to(config.DEVICE)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            y_logits = model(x)\n",
    "            y_prob = torch.sigmoid(y_logits).squeeze().cpu().numpy()\n",
    "        \n",
    "        y_true = y_true.cpu().numpy()\n",
    "        positions = np.linspace(0, 5, len(y_true))  \n",
    "\n",
    "        axes[i, 0].fill_between(positions, 0, y_true, alpha=0.6, color='red', label='Real Damage')\n",
    "        axes[i, 0].plot(positions, y_true, 'r-', linewidth=1.5)\n",
    "        axes[i, 0].set_xlabel('Position (m)')\n",
    "        axes[i, 0].set_ylabel('Damage label')\n",
    "        axes[i, 0].set_title(f'sample {idx} - real label')\n",
    "        axes[i, 0].legend(loc='upper right')\n",
    "        axes[i, 0].grid(True, alpha=0.3)\n",
    "        axes[i, 0].set_ylim([-0.1, 1.2])\n",
    "\n",
    "        axes[i, 1].plot(positions, y_prob, 'b-', linewidth=2, label='Damage probability')\n",
    "        axes[i, 1].axhline(y=0.5, color='k', linestyle='--', alpha=0.5, label='threshold value (0.5)')\n",
    "\n",
    "        pred_damage = (y_prob > 0.5).astype(float)\n",
    "        axes[i, 1].fill_between(positions, 0, pred_damage, alpha=0.4, color='blue', label='Predication Damage')\n",
    "        \n",
    "        axes[i, 1].set_xlabel('Position (m)')\n",
    "        axes[i, 1].set_ylabel('Damage probability')\n",
    "        axes[i, 1].set_title(f'sample {idx} - model prediction')\n",
    "        axes[i, 1].legend(loc='upper right')\n",
    "        axes[i, 1].grid(True, alpha=0.3)\n",
    "        axes[i, 1].set_ylim([-0.1, 1.2])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plot_path = config.SAVE_DIR / '1DCNN_test_predictions.png'\n",
    "    plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"æµ‹è¯•é›†é¢„æµ‹å¯è§†åŒ–å·²ä¿å­˜: {plot_path}\")\n",
    "\n",
    "\n",
    "def plot_training_curves(history, save_dir):\n",
    "\n",
    "    epochs = history['epoch']\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    fig.suptitle(f'å®Œæ•´è§„æ¨¡è®­ç»ƒæ›²çº¿ (90kè®­ç»ƒ + 15kéªŒè¯)', fontsize=16, y=1.02)\n",
    "    \n",
    "\n",
    "    ax = axes[0, 0]\n",
    "    ax.plot(epochs, history['train_loss'], 'b-', linewidth=2, label='Training lose')\n",
    "    ax.plot(epochs, history['val_loss'], 'r-', linewidth=2, label='Verification lose')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Lose')\n",
    "    ax.set_title('Training and Verification lose')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "\n",
    "    ax = axes[0, 1]\n",
    "    ax.plot(epochs, history['train_acc'], 'b-', linewidth=2, label='Training Accuracy')\n",
    "    ax.plot(epochs, history['val_acc'], 'r-', linewidth=2, label='Verification Accuracy')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title('Training and Verification accuracy')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim([0.5, 1.0])\n",
    "    \n",
    "\n",
    "    ax = axes[0, 2]\n",
    "    ax.plot(epochs, history['learning_rate'], 'g-', linewidth=2)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Learning rate')\n",
    "    ax.set_title('Change of the Learning rate')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_yscale('log')\n",
    "    \n",
    "\n",
    "    ax = axes[1, 0]\n",
    "    ax.plot(epochs, history['epoch_time'], 'm-', linewidth=2)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Time (ç§’)')\n",
    "    ax.set_title('Each Epoch training time')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.axhline(y=np.mean(history['epoch_time']), color='k', linestyle='--', \n",
    "               label=f'å¹³å‡: {np.mean(history[\"epoch_time\"]):.1f}s')\n",
    "    ax.legend()\n",
    "\n",
    "    ax = axes[1, 1]\n",
    "    scatter = ax.scatter(history['val_loss'], history['val_acc'], \n",
    "                        c=history['epoch'], cmap='viridis', \n",
    "                        s=30, alpha=0.7)\n",
    "    ax.set_xlabel('Verification lose')\n",
    "    ax.set_ylabel('Verification Accuracy')\n",
    "    ax.set_title('Lose vs. Verification Accuracy')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.colorbar(scatter, ax=ax, label='Epoch')\n",
    "\n",
    "    ax = axes[1, 2]\n",
    "    early_epochs = min(30, len(epochs))\n",
    "    ax.plot(epochs[:early_epochs], history['train_loss'][:early_epochs], 'b-o', \n",
    "            linewidth=1.5, markersize=3, label='Training lose')\n",
    "    ax.plot(epochs[:early_epochs], history['val_loss'][:early_epochs], 'r-s', \n",
    "            linewidth=1.5, markersize=3, label='Verification lose')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('lose')\n",
    "    ax.set_title(f'early training (first {early_epochs} Epoch)')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plot_path = save_dir / '1DCNN_training_curves.png'\n",
    "    plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"è®­ç»ƒæ›²çº¿å·²ä¿å­˜: {plot_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 80)\n",
    "    print(\"æ¢ç»“æ„æŸä¼¤è¯†åˆ« - å®Œæ•´è§„æ¨¡è®­ç»ƒ (1D CNN)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    try:\n",
    "\n",
    "        model, history, best_results = train_full_scale()\n",
    "\n",
    "        best_model_path = Path(f'./results/full_{int(time.time())}') / 'best_model.pth'\n",
    "        if best_model_path.exists():\n",
    "            test_results = evaluate_on_testset(best_model_path)\n",
    "            \n",
    "            print(\"\\n\" + \"=\" * 80)\n",
    "            print(\"ğŸ‰ å…¨éƒ¨æµç¨‹å®Œæˆ!\")\n",
    "            print(\"=\" * 80)\n",
    "            print(f\"æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {test_results['test_accuracy']:.4f}\")\n",
    "            print(f\"æµ‹è¯•F1åˆ†æ•°: {test_results['test_damage_f1']:.4f}\")\n",
    "\n",
    "            medium_acc = 0.82  \n",
    "            improvement = test_results['test_accuracy'] - medium_acc\n",
    "            \n",
    "            print(f\"\\n æ€§èƒ½å¯¹æ¯”:\")\n",
    "            print(f\"   ä¸­ç­‰è§„æ¨¡FCN (10kæ•°æ®): {medium_acc:.2%}\")\n",
    "            print(f\"   å®Œæ•´è§„æ¨¡CNN (90kæ•°æ®): {test_results['test_accuracy']:.2%}\")\n",
    "            \n",
    "            if improvement > 0:\n",
    "                print(f\"    æ€§èƒ½æå‡: +{improvement*100:.1f}%\")\n",
    "            else:\n",
    "                print(f\"     æ€§èƒ½å˜åŒ–: {improvement*100:.1f}%\")\n",
    "        else:\n",
    "            print(f\"  æœ€ä½³æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°: {best_model_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "        error_dir = Path('./training_errors')\n",
    "        error_dir.mkdir(exist_ok=True)\n",
    "        error_file = error_dir / f'error_{int(time.time())}.txt'\n",
    "        \n",
    "        with open(error_file, 'w') as f:\n",
    "            f.write(f\"è®­ç»ƒé”™è¯¯: {e}\\n\")\n",
    "            f.write(f\"æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "            f.write(\"\\nTraceback:\\n\")\n",
    "            f.write(traceback.format_exc())\n",
    "        \n",
    "        print(f\" é”™è¯¯è¯¦æƒ…å·²ä¿å­˜: {error_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Torch GPU)",
   "language": "python",
   "name": "torch_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
